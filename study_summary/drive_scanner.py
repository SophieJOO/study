"""
Drive ìŠ¤ìº” ëª¨ë“ˆ - Apps Script ì›¹ì•± ì—°ë™
- Apps Scriptê°€ ë§¤ì¼ ìƒì„±í•˜ëŠ” digest HTMLì„ ê°€ì ¸ì™€ íšŒì›ë³„ í•™ìŠµ ë‚´ìš© ì¶”ì¶œ
"""
import json
import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from pathlib import Path

import requests
from bs4 import BeautifulSoup

from config import APPS_SCRIPT_URL, DEADLINE_HOUR

# members.json ê²½ë¡œ
MEMBERS_FILE = Path(__file__).parent / "members.json"


def get_target_date() -> str:
    """
    ëŒ€ìƒ ë‚ ì§œ ê³„ì‚° (ìƒˆë²½ 3ì‹œ ê¸°ì¤€)
    - í˜„ì¬ ì‹œê°„ì´ ìƒˆë²½ 3ì‹œ ì´ì „ì´ë©´ ì „ë‚  ë‚ ì§œ
    - ìƒˆë²½ 3ì‹œ ì´í›„ë©´ ì˜¤ëŠ˜ ë‚ ì§œ
    """
    now = datetime.now()
    if now.hour < DEADLINE_HOUR:
        target = now - timedelta(days=1)
    else:
        target = now
    return target.strftime("%Y-%m-%d")


def _load_member_names() -> List[str]:
    """members.jsonì—ì„œ í™œì„± íšŒì› ì´ë¦„ ëª©ë¡ ë¡œë“œ"""
    if not MEMBERS_FILE.exists():
        return []
    with open(MEMBERS_FILE, "r", encoding="utf-8") as f:
        data = json.load(f)
    return [
        m["name"]
        for m in data.get("members", [])
        if m.get("active") and m.get("name")
    ]


def _extract_inner_html(raw_html: str) -> str:
    """
    Apps Script HtmlServiceì˜ iframe wrapperì—ì„œ ì‹¤ì œ ì½˜í…ì¸  HTMLì„ ì¶”ì¶œ.
    Googleì€ doGet ì‘ë‹µì„ sandboxFrame iframe ì•ˆì— ì´ìŠ¤ì¼€ì´í”„ëœ JS ë¬¸ìì—´ë¡œ ê°ì‹¼ë‹¤.
    """
    # ìŠ¤í¬ë¦½íŠ¸ ë¸”ë¡ì—ì„œ ê°€ì¥ ê¸´ ë¬¸ìì—´ ë¦¬í„°ëŸ´ì„ ì°¾ìŒ (= ì‹¤ì œ HTML)
    script_match = re.search(r'<script[^>]*>(.*?)</script>\s*</body>', raw_html, re.DOTALL)
    if not script_match:
        return raw_html  # wrapperê°€ ì•„ë‹ˆë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜

    script = script_match.group(1)
    strings = re.findall(r'"([^"]{500,})"', script)
    if not strings:
        return raw_html

    longest = max(strings, key=len)

    # JS ì´ì¤‘ ì´ìŠ¤ì¼€ì´í”„ ë””ì½”ë”©
    # Googleì˜ iframe wrapperëŠ” HTMLì„ JS ë¬¸ìì—´ë¡œ 2ì¤‘ ì´ìŠ¤ì¼€ì´í”„í•¨
    decoded = longest
    # 1ì°¨: \\x â†’ \x, \\" â†’ \", \\\\ â†’ \\, \\n â†’ \n, \\/ â†’ \/
    decoded = re.sub(r'\\\\x([0-9a-fA-F]{2})', lambda m: chr(int(m.group(1), 16)), decoded)
    decoded = decoded.replace('\\\\"', '"')
    decoded = decoded.replace('\\\\/', '/')
    decoded = decoded.replace('\\\\n', '\n')
    decoded = decoded.replace('\\\\t', '\t')
    decoded = decoded.replace('\\\\\\\\', '\\')
    # 2ì°¨: ë‚¨ì€ ë‹¨ì¼ ì´ìŠ¤ì¼€ì´í”„
    decoded = decoded.replace('\\n', '\n')
    decoded = decoded.replace('\\t', '\t')
    decoded = decoded.replace('\\/', '/')
    decoded = decoded.replace('\\"', '"')
    decoded = re.sub(r'\\x([0-9a-fA-F]{2})', lambda m: chr(int(m.group(1), 16)), decoded)
    decoded = decoded.replace('\\\\', '\\')

    # JSON ì„¤ì • ë¶€ë¶„ì„ ê±´ë„ˆë›°ê³  ì‹¤ì œ HTML ì‹œì‘ì  ì°¾ê¸°
    html_start = decoded.find('<!DOCTYPE')
    if html_start < 0:
        html_start = decoded.find('<html')
    if html_start < 0:
        html_start = decoded.find('<')
    if html_start < 0:
        return raw_html

    return decoded[html_start:]


def fetch_digest_html(date: str) -> str:
    """
    Apps Script ì›¹ì•±ì—ì„œ digest HTML ê°€ì ¸ì˜¤ê¸°

    Args:
        date: YYYY-MM-DD í˜•ì‹ ë‚ ì§œ

    Returns:
        ì‹¤ì œ ì½˜í…ì¸  HTML ë¬¸ìì—´ (iframe wrapper ì œê±°ë¨)

    Raises:
        RuntimeError: ìš”ì²­ ì‹¤íŒ¨ ì‹œ
    """
    if not APPS_SCRIPT_URL:
        raise RuntimeError("APPS_SCRIPT_URLì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")

    url = f"{APPS_SCRIPT_URL}?date={date}"
    resp = requests.get(url, timeout=30)
    resp.raise_for_status()
    return _extract_inner_html(resp.text)


def parse_digest_html(html: str) -> List[Dict]:
    """
    digest HTMLì„ íŒŒì‹±í•˜ì—¬ íšŒì›ë³„ ë°ì´í„° ì¶”ì¶œ

    Args:
        html: Apps Scriptê°€ ìƒì„±í•œ digest HTML

    Returns:
        [{"name": str, "text_content": str, "files": [str, ...]}, ...]
    """
    soup = BeautifulSoup(html, "html.parser")
    members = []

    for section in soup.select(".member-section"):
        # íšŒì› ì´ë¦„
        h2 = section.find("h2")
        if not h2:
            continue
        name = h2.get_text(strip=True)

        # íŒŒì¼ ëª©ë¡
        files = []
        for li in section.select(".file-list li"):
            files.append(li.get_text(strip=True))

        # í•™ìŠµ ë‚´ìš© í…ìŠ¤íŠ¸
        content_body = section.select_one(".content-body")
        text_content = content_body.get_text(separator="\n", strip=True) if content_body else ""

        members.append({
            "name": name,
            "text_content": text_content,
            "files": files,
        })

    return members


def scan_all_members(target_date: Optional[str] = None) -> List[Dict]:
    """
    ëª¨ë“  íšŒì›ì˜ í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘ (Apps Script ì›¹ì•± ê²½ìœ )

    Returns:
        ê¸°ì¡´ê³¼ ë™ì¼í•œ í˜•ì‹:
        [{"name", "date", "has_submission", "text_content", "files"}, ...]
    """
    if target_date is None:
        target_date = get_target_date()

    print(f"ğŸ“… ëŒ€ìƒ ë‚ ì§œ: {target_date}")

    # 1) ì›¹ì•±ì—ì„œ HTML ê°€ì ¸ì˜¤ê¸°
    print("  ğŸŒ Apps Script ì›¹ì•±ì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
    html = fetch_digest_html(target_date)

    # 2) HTML íŒŒì‹± â†’ ì œì¶œí•œ íšŒì› ë°ì´í„°
    parsed = parse_digest_html(html)
    submitted_names = {m["name"] for m in parsed}
    print(f"  ğŸ“Š HTMLì—ì„œ {len(parsed)}ëª… ë°ì´í„° íŒŒì‹± ì™„ë£Œ")

    # 3) ê²°ê³¼ ì¡°ë¦½
    results = []
    for m in parsed:
        results.append({
            "name": m["name"],
            "date": target_date,
            "has_submission": True,
            "text_content": m["text_content"],
            "files": [{"name": f, "type": "unknown"} for f in m["files"]],
        })

    # 4) members.jsonì—ì„œ ë¯¸ì œì¶œ íšŒì› ì¶”ê°€
    all_names = _load_member_names()
    for name in all_names:
        if name not in submitted_names:
            results.append({
                "name": name,
                "date": target_date,
                "has_submission": False,
                "text_content": "",
                "files": [],
            })

    # 5) ìš”ì•½ ì¶œë ¥
    submitted_count = sum(1 for r in results if r["has_submission"])
    print(f"  ğŸ‘¥ ì „ì²´: {len(results)}ëª… (ì œì¶œ {submitted_count} / ë¯¸ì œì¶œ {len(results) - submitted_count})")
    for r in results:
        status = "âœ…" if r["has_submission"] else "âŒ"
        print(f"    {status} {r['name']}")

    return results


def test_connection() -> bool:
    """Apps Script ì›¹ì•± ì—°ê²° í…ŒìŠ¤íŠ¸"""
    try:
        if not APPS_SCRIPT_URL:
            print("âŒ APPS_SCRIPT_URLì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return False

        print(f"  URL: {APPS_SCRIPT_URL}")
        target_date = get_target_date()
        html = fetch_digest_html(target_date)

        parsed = parse_digest_html(html)
        print(f"âœ… Apps Script ì›¹ì•± ì—°ê²° ì„±ê³µ! ({len(parsed)}ëª… ë°ì´í„° ìˆ˜ì‹ )")
        return True
    except Exception as e:
        print(f"âŒ Apps Script ì›¹ì•± ì—°ê²° ì‹¤íŒ¨: {e}")
        return False


if __name__ == "__main__":
    print("=== Apps Script ì›¹ì•± ì—°ê²° í…ŒìŠ¤íŠ¸ ===")
    if test_connection():
        print("\n=== íšŒì› ë°ì´í„° ìŠ¤ìº” í…ŒìŠ¤íŠ¸ ===")
        results = scan_all_members()

        print("\n=== ìŠ¤ìº” ê²°ê³¼ ===")
        for r in results:
            status = "âœ…" if r["has_submission"] else "âŒ"
            files_count = len(r["files"])
            content_len = len(r["text_content"])
            print(f"{status} {r['name']}: {files_count}ê°œ íŒŒì¼, {content_len}ì í…ìŠ¤íŠ¸")
